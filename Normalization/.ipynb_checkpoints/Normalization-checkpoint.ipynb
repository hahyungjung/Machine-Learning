{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90252449",
   "metadata": {},
   "source": [
    "https://hleecaster.com/ml-normalization-concept/  \n",
    "https://deepchecks.com/glossary/normalization-in-machine-learning/  \n",
    "\n",
    "Normalization is a data preparation technique that is frequently used in machine learning. The process of transforming the columns in a dataset to the same scale is referred to as normalization. Every dataset does not need to be normalized for machine learning. It is only required when the ranges of characteristics are different.  \n",
    "  \n",
    "### Normalization techniques in Machine Learning  \n",
    "\n",
    "If you’re new to data science and machine learning, you’ve certainly questioned a lot about what feature normalization in machine learning is and how it works.  \n",
    "\n",
    "The most widely used types of normalization in machine learning are:  \n",
    "\n",
    "#### Min-Max Scaling –  \n",
    "Subtract the minimum value from each column’s highest value and divide by the range. Each new column has a minimum value of 0 and a maximum value of 1.  \n",
    "\n",
    "#### Standardization Scaling –    \n",
    "The term “standardization” refers to the process of centering a variable at zero and standardizing the variance at one. Subtracting the mean of each observation and then dividing by the standard deviation is the procedure:  \n",
    "\n",
    "The features will be rescaled so that they have the attributes of a typical normal distribution with standard deviations.  \n",
    "\n",
    "\n",
    "### When to use normalization and standardization  \n",
    "\n",
    "When you don’t know the distribution of your data or when you know it’s not Gaussian, normalization is a smart approach to apply. Normalization is useful when your data has variable scales and the technique you’re employing, such as k-nearest neighbors and artificial neural networks, doesn’t make assumptions about the distribution of your data.  \n",
    "\n",
    "The assumption behind standardization is that your data follows a Gaussian (bell curve) distribution. This isn’t required, however, it helps the approach work better if your attribute distribution is Gaussian. When your data has variable dimensions and the technique you’re using (like logistic regression,  linear regression, linear discriminant analysis) standardization is useful.  \n",
    "  \n",
    "#### 요약\n",
    "최소 최대 정규화: 모든 feature들의 스케일이 동일하지만, 이상치(outlier)를 잘 처리하지 못한다.  \n",
    "Z-점수 정규화 : 이상치(outlier)를 잘 처리하지만, 정확히 동일한 척도로 정규화 된 데이터를 생성하지는 않는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    normalized = []\n",
    "    \n",
    "    for value in lst:\n",
    "        normalized_num = (value - min(lst)) / (max(lst) - min(lst))\n",
    "        normalized.append(normalized_num)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(lst):\n",
    "    normalized = []\n",
    "    for value in lst:\n",
    "        normalized_num = (value - np.mean(lst)) / np.std(lst)\n",
    "        normalized.append(normalized_num)\n",
    "    return normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
